{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "from itertools import islice\n",
    "\n",
    "def clean_text(text):\n",
    "    return re.sub(\"[^a-zа-я]\", \"\", text.lower())\n",
    "\n",
    "def entropy(text):\n",
    "    frequency = Counter(text)\n",
    "    total_characters = len(text)\n",
    "    probabilities = [freq / total_characters for freq in frequency.values()]\n",
    "    return -sum(p * math.log2(p) for p in probabilities)\n",
    "\n",
    "def average_code_length_uniform(alphabet_size):\n",
    "    return math.log2(alphabet_size)\n",
    "\n",
    "def redundancy(entropy, avg_code_length):\n",
    "    return avg_code_length - entropy\n",
    "\n",
    "def shannon_fano_coding(freq_table):\n",
    "    sorted_freq = sorted(freq_table.items(), key=lambda item: item[1], reverse=True)\n",
    "    return _shannon_fano_recursive(sorted_freq)\n",
    "\n",
    "def _shannon_fano_recursive(sorted_freq):\n",
    "    if len(sorted_freq) == 1:\n",
    "        return {sorted_freq[0][0]: \"\"}\n",
    "    total_sum = sum([item[1] for item in sorted_freq])\n",
    "    cumulative_sum = 0\n",
    "    split_index = 0\n",
    "    for i, item in enumerate(sorted_freq):\n",
    "        cumulative_sum += item[1]\n",
    "        if cumulative_sum >= total_sum / 2:\n",
    "            split_index = i + 1\n",
    "            break\n",
    "    left_part = _shannon_fano_recursive(sorted_freq[:split_index])\n",
    "    right_part = _shannon_fano_recursive(sorted_freq[split_index:])\n",
    "    left_part = {key: \"0\" + code for key, code in left_part.items()}\n",
    "    right_part = {key: \"1\" + code for key, code in right_part.items()}\n",
    "    left_part.update(right_part)\n",
    "    return left_part\n",
    "\n",
    "def encode_n_grams(text, code_table, n=2):\n",
    "    n_grams = [''.join(islice(text, i, i + n)) for i in range(0, len(text) - n + 1, n)]\n",
    "    return ''.join(code_table[gram] for gram in n_grams if gram in code_table)\n",
    "\n",
    "def decode_n_grams(encoded_text, code_table):\n",
    "    reverse_table = {v: k for k, v in code_table.items()}\n",
    "    decoded_text = []\n",
    "    buffer = \"\"\n",
    "    for bit in encoded_text:\n",
    "        buffer += bit\n",
    "        if buffer in reverse_table:\n",
    "            decoded_text.append(reverse_table[buffer])\n",
    "            buffer = \"\"\n",
    "    return ''.join(decoded_text)\n",
    "\n",
    "def get_frequency_table(text, n=1):\n",
    "    n_grams = [''.join(islice(text, i, i + n)) for i in range(len(text) - n + 1)]\n",
    "    return Counter(n_grams)\n",
    "\n",
    "def average_code_length(freq_table, code_table):\n",
    "    total_symbols = sum(freq_table.values())\n",
    "    return sum(len(code_table[symbol]) * freq / total_symbols for symbol, freq in freq_table.items())\n",
    "\n",
    "def compression_efficiency(entropy, avg_code_length):\n",
    "    return entropy / avg_code_length\n",
    "\n",
    "text = \"this is a sample text to be encoded using shannon fano coding method.\"\n",
    "\n",
    "cleaned_text = clean_text(text)\n",
    "\n",
    "text_entropy = entropy(cleaned_text)\n",
    "alphabet_size = len(set(cleaned_text))\n",
    "avg_code_length_uniform = average_code_length_uniform(alphabet_size)\n",
    "text_redundancy = redundancy(text_entropy, avg_code_length_uniform)\n",
    "\n",
    "freq_table_single = get_frequency_table(cleaned_text, n=1)\n",
    "shannon_fano_single = shannon_fano_coding(freq_table_single)\n",
    "encoded_single = encode_n_grams(cleaned_text, shannon_fano_single, n=1)\n",
    "decoded_single = decode_n_grams(encoded_single, shannon_fano_single)\n",
    "avg_code_length_single = average_code_length(freq_table_single, shannon_fano_single)\n",
    "compression_efficiency_single = compression_efficiency(text_entropy, avg_code_length_single)\n",
    "\n",
    "freq_table_double = get_frequency_table(cleaned_text, n=2)\n",
    "shannon_fano_double = shannon_fano_coding(freq_table_double)\n",
    "encoded_double = encode_n_grams(cleaned_text, shannon_fano_double, n=2)\n",
    "decoded_double = decode_n_grams(encoded_double, shannon_fano_double)\n",
    "avg_code_length_double = average_code_length(freq_table_double, shannon_fano_double)\n",
    "compression_efficiency_double = compression_efficiency(text_entropy, avg_code_length_double)\n",
    "\n",
    "results = {\n",
    "    'Энтропия': text_entropy,\n",
    "    'Средняя длина кода (равномерная)': avg_code_length_uniform,\n",
    "    'Избыточность': text_redundancy,\n",
    "    'Средняя длина кода (одиночный)': avg_code_length_single,\n",
    "    'Эффективность сжатия (одиночная)': compression_efficiency_single,\n",
    "    'Средняя длина кода (двойная)': avg_code_length_double,\n",
    "    'Эффективность сжатия (двойная)': compression_efficiency_double,\n",
    "    'Закодированный текст (одинарный)': encoded_single,\n",
    "    'Расшифрованный текст (одиночный)': decoded_single,\n",
    "    'Закодированный текст (двойной)': encoded_double,\n",
    "    'Расшифрованный текст (двойной)': decoded_double,\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(list(results.items()), columns=['Metric', 'Value'])\n",
    "import ace_tools_open as tools; tools.display_dataframe_to_user(name=\"Text Compression Results\", dataframe=results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
